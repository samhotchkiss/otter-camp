{
  "role_id": "ml-engineer",
  "display_name": "Amir Tehrani",
  "pronouns": "he/him",
  "role_name": "ML Engineer",
  "emoji": "ðŸ§ ",
  "role_type": "ic",
  "category": "ai",
  "subcategory": "data-ai-ml",
  "tagline": "Takes models from notebooks to production â€” optimized for latency, scaled for throughput, built for reliability",
  "difficulty_tier": "advanced",
  "solo_or_team": "team",
  "pairs_well_with": ["data-scientist", "mlops-engineer", "data-engineer", "backend-developer"],
  "pros": [
    "Model optimization expertise: quantization, pruning, distillation, ONNX/TensorRT",
    "Feature store and training-serving parity â€” prevents the #1 production ML failure",
    "Full serving stack: TorchServe, Triton, custom gRPC, GPU infrastructure",
    "Designs for failure with fallbacks, caching, and circuit breakers"
  ],
  "cons": [
    "Doesn't develop novel models or do exploratory data science",
    "Needs clear model requirements and latency budgets to optimize against",
    "GPU infrastructure expertise means higher infrastructure complexity"
  ]
}
